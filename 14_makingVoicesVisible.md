---
title: Making Voices Visible
layout: default
---

# Making voice visible

## Or why we should investigate the  anatomy of AI and networked devices

#### Vladan Joler

We, as the human particle of networked society, are visible in almost every moment of interaction with technology. We are constantly being tracked, quantified, analysed and commodified. But in contrast to our hyper visibility, many of the phases of birth, life and death of networked devices are covered with a cloak of invisibility. With emerging popular devices relying on a centralised AI infrastructure and invisible interfaces such as voice for example, it seems that the cloak of invisibility is growing even bigger.

We have reached a point where the smooth and shiny aluminium surface of our devices is screwless, without visible entrance, we have lost the keyboards allowing us to code and create new worlds. And now we are losing visible interfaces that will be replaced with voice or gesture interfaces. With the so called “cloud” and machine learning technologies computing and processing does not happen within our devices anymore, but in some big gray building thousands of kilometers away. The actual devices, processing software and algorithms are  somewhere else, or possibly on many places at the same time, but rarely within our reach. What we have for example in devices such as Amazon Echo is just a few sensors and speakers. Not a lot to own on the user’s side, not a lot to open, brake, fix or remix.

**Many layers of invisibility**

As is the case with networked devices we can also speak about various other layers of invisibility:

+ Invisibility of the network infrastructure and the so called cloud infrastructure;
+ Invisibility of the production process, supply chains and materials being used in production;
+ Invisibility of human labour related to many processes in the production of devices;
+ Invisibility of the code hidden behind proprietary software;
+ Invisibility of the overall energy consumption of the whole system.

And many other obvious or more subtle forms of invisibility that we have already related to networked devices. There are some forms of invisibility that are specific and emerging when we speak about voice interfaces as an extension to AI and machine learning infrastructures in comparison to the network infrastructure that we have investigated in our previous work.

On one hand, at the front we have basically invisible interfaces, but then,  deeper at the level of the heart of the process, in the darkness of the neural networks and machine learning, we are seeing new forms of invisibility. Invisibility of the process of machine deduction itself. In previous years in academic, policy and activist circles there was a discourse related to the importance of algorithmic transparency. We can't say that some clear solution emerged from those discussions; and the problem of algorithmic transparency is still very much on the table. But, if we speak about machine learning algorithms, that are responsible for models behind voice interfaces for example, we are facing potentially even deeper problem.

By its nature, deep learning is a particularly dark black box[<sup>1</sup>](#fn1)<a id="fnref1"></a> and it is hard to believe that we will be able to audit why one neural network deducted something or let say discriminated someone by looking into the hidden layers or nodes of the neural network. Those processes will probably stay hidden within hidden layers of the neural network. This phenomenon has a name: The Black Box of AI. Once one neural network is trained, it is really hard to understand why it gives a particular result to a set of data inputs. This can be a critical point or obstacle in building trust in AI judgment and handful of research institutions and academic researchers are exploring this topic[<sup>2</sup>](#fn2)<a id="fnref2"></a>. 

But probably our attention in the process of revealing layers of invisibility should not be just on the layer of neural networks and deep learning itself but equally on the input layer, or let's say schoolbooks, data sets or feeds of information that one AI reads. It looks like it has never been easier to build AI or machine learning-based systems than it is today. Availability of open-source tools for doing that in combination with easy accessible computation power through cloud oligarchs such as Amazon (AWS) or Google (Google Cloud) is giving a fake idea of accessibility, openness and decentralisation of AI revolution.

We can say that having your own neural network is becoming more and more accessible from the point of setting up your own system, but when it comes to datasets needed for teaching those systems we are facing many new problems. Training datasets became the ultimate resource for development of future machine learning or AI and they are heavily protected behind the proprietary walls of handful of usual suspects. Here we are seeing the same concentration of power accumulated through surveillance economy and massive data hoarding practices in the last decade becoming crucial resource for development of new even more asymmetric power and creation of even bigger gap between the big five and the rest. In their system of immaterial labour exploitation, our new task is to feed their neural networks with our behavioral data, voice, tagged pictures and videos or medical data.

![Image](Images/14_MVV_Image1.jpg)
_Lunch atop of Skyscraper. The glass negative of this picture is now owned by Corbis, who acquired it from the Acme Newspictures archive in 1995. The negative at some point long ago was broken into five pieces_




